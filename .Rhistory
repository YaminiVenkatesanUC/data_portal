# RTD sales (nominal) ART9ZQ
rtd_nom<-tsmfetch(conn,"RTTQ.SF91CS") %>% rename(nom=value)
rtd_nom$nom<-round(rtd_nom$nom,1)
# RTD sales (real) ART9_ZQ
rtd_real<-tsmfetch(conn,"RTTQ.SF91KS") %>% rename(real=value)
rtd_real$real<-round(rtd_real$real,1)
# ART9ZQ/ART9_ZQ * ECT Nominal = ECT Real
# (if no deflator, use the monthly change from the previous year)
ect_deflator<-merge(rtd_nom,rtd_real) %>% mutate(deflate=nom/real)
ECT_real_in<-merge(ECT,ect_deflator,all.x=TRUE) %>% arrange(Date)
for(i in (nrow(ECT_real_in)-12):nrow(ECT_real_in)){
#Check for missing deflator. Should only be missing a few months until the next quarter is out, but check back over the past year
if(is.na(ECT_real_in[i,]$deflate)){
ECT_real_in[i,]$deflate<-ECT_real_in[i-12,]$deflate*(ECT_real_in[i-1,]$deflate/ECT_real_in[i-13,]$deflate)
}
}
ECT_real_in$ect_real<-ECT_real_in$ECT_spend/ECT_real_in$deflate
ECT_real<-ECT_real_in %>% select(Date,ect_real)
###############################
### Traffic
traffic<-read_excel(paste0(nzacdir,"ANZ data/ANZ Truckometer.xlsx"), sheet = "Data",skip=1)
traffic_portal<-traffic%>%select(Date=...1,heavy_traffic= `ANZ heavy traffic index...2`, light_traffic= `ANZ light traffic index...3`) %>%
mutate(Date=ymd(Date))
traffic_data<-traffic_portal
###############################
### Electricity Grid Demand
grid<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20040101&RegionType=NZ&TimeScale=MONTH&Source=RM",skip="Period start")
finaldemand<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20090101&RegionType=NZ&TimeScale=MONTH",skip="Period start")
electricity_back<-read.csv(paste0(backcast, "grid_demand.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
grid_electricity<-grid%>%
select(Date=`Period start`,grid_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date))
final_electricity<-finaldemand%>%
select(Date=`Period start`,final_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date)) %>% filter(Date>as.Date("2009-07-01"))
newdata<-data.frame()
if(max(final_electricity$Date)>max(grid_electricity$Date)){
# If there is a more recent month of Final Demand data available, we use linear regression to predict a grid demand figure
relate_data<-merge(grid_electricity,final_electricity)
relation<-lm(grid_demand~final_demand,data=relate_data)
newdata<-final_electricity[final_electricity$Date>max(grid_electricity$Date),]
newdata$grid_demand<-predict(relation,newdata)
newdata<-newdata %>% select(Date,grid_demand)
}
grid_electricity<-rbind(electricity_back,grid_electricity,newdata)
###############################
### Seek Job Ads
# Get latest file in directory
df <- file.info(list.files(paste0(nzacdir,"SEEK data"), full.names = T))
latest<-rownames(df)[which.max(df$mtime)]
jobs<-read_excel(latest, sheet = 1,skip=2)
jobs_back<-read.csv(paste0(backcast, "seek_job_ads.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
job_listing<-jobs%>%
select(Date, sk_job_ads=`NZ Job ad Index`)%>%
mutate(Date=ymd(Date))
# Need to rescale old job series based on (seasonally adjusted) first figure of more recent series
deflator<-jobs_back[jobs_back$Date==max(jobs_back$Date),]$sk_job_ads/
job_listing[job_listing$Date==min(job_listing$Date),]$sk_job_ads
jobs_back$sk_job_ads<-jobs_back$sk_job_ads/deflator
jobs_back<-jobs_back[!(jobs_back$Date==max(jobs_back$Date)),]
job_listing<-rbind(jobs_back,job_listing)
###############################
### Job Seeker
jobseek<-read_excel(paste0(dir,"COVID-19 MSD Jobseeker Support.xlsx"), sheet = 1)
jobseek_back<-read.csv(paste0(backcast, "jobseekers.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=ymd(Date),jobseekers=as.numeric(gsub(",", "", jobseekers)))
## Get Last Friday of Each Month Dates, 200 gives us until 2035
date.start.month=seq(as.Date("2020-01-01"),length=200,by="months")
date.end.month <- ceiling_date(date.start.month, "month")-1
Friday<-data.frame(
next_friday = ceiling_date(date.end.month, "week") + ifelse(weekdays(date.end.month) %in% c("Saturday", "Sunday"), 5, -2),
Date=rollbackward(date.end.month,roll_to_first = TRUE)
)
## Only get Last Friday numbers, and only for new rows
job_seekers_in<-jobseek%>%
select(next_friday='date', jobseekers=`Jobseeker Support - Work Ready`)%>%
mutate(next_friday=ymd(next_friday))
job_seekers<-merge(job_seekers_in,Friday) %>% select(Date,jobseekers) %>% filter(Date>max(jobseek_back$Date))
job_seekers<-rbind(jobseek_back,job_seekers)
## Update the backseries (as MSD only gives a Window of time)
write.csv(job_seekers,paste0(backcast, "jobseekers.csv"),row.names = FALSE)
#########################################
## FINAL DATA MERGE
all_data_merge<-Reduce(function(x,y) merge(x, y, all =T),list(bnz_pmi,anz_activity_outlook, job_seekers,
grid_electricity, traffic_data, job_listing, ECT_real))
#write_xlsx(all_data, paste0(nzacdir,"NZAC portal data ", Sys.Date(), ".xlsx"))
## SET UP LIBRARIES
library(writexl)
library(readxl)
library(dplyr)
library(tidyr)
library(magrittr)
library(lubridate)
library(data.table)
#library(nipals)
source("tsmfetch.R")
options(digits=10)
#If loaded libraries in read_in.R, need to add this.
library(zoo)
dir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/"
nzacdir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/"
#meta<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/Raymund/META data/"
backcast<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/Backcast/"
## CREATE NZAC
all_data_merge<-Reduce(function(x,y) merge(x, y, all =T),list(bnz_pmi,anz_activity_outlook, job_seekers,
grid_electricity, traffic_data, job_listing, ECT_real))
all_data_merge$heavy_traffic<-round(all_data_merge$heavy_traffic,2)
all_data_merge$light_traffic<-round(all_data_merge$light_traffic,2)
# Create APC
all_data_merge$l_ect_real<-all_data_merge$ect_real/lag(all_data_merge$ect_real,12)-1
all_data_merge$l_heavy_traffic<-all_data_merge$heavy_traffic/lag(all_data_merge$heavy_traffic,12)-1
all_data_merge$l_light_traffic<-all_data_merge$light_traffic/lag(all_data_merge$light_traffic,12)-1
all_data_merge$l_grid_demand<-all_data_merge$grid_demand/lag(all_data_merge$grid_demand,12)-1
all_data_merge$l_jobseekers<-all_data_merge$jobseekers/lag(all_data_merge$jobseekers,12)-1
all_data_merge$l_sk_job_ads<-all_data_merge$sk_job_ads/lag(all_data_merge$sk_job_ads,12)-1
nzac<-all_data_merge %>% select(Date,l_ect_real,l_heavy_traffic,l_light_traffic,pmi,activity_outlook,l_grid_demand,l_jobseekers,l_sk_job_ads)
nzac<-nzac[nzac$Date>=as.Date("2003-10-01"),]
#nzac<-nzac[complete.cases(nzac),]
# Do PCA
#if(sum(is.na(nzac))>0){
#  nzac.pca <- empca(nzac[,c(2:9)], maxiter = 3000, tol=1e-5)
#  nzac.pc1<-data.frame(Date=nzac$Date,value=-nzac.pca$scores[,'PC1'])
#}else{
#}
nzac.pca <- prcomp(nzac[,c(2:9)], center = TRUE,scale. = TRUE)
nzac.pc1<-data.frame(Date=nzac$Date,value=-nzac.pca$x[,'PC1'])
# Does normalise do anything?
nzac.norm<-scale(nzac[,-c(1)])
# write.csv(nzac.norm,"normvalues.csv")
# nzac.norm.pca <- prcomp(nzac.norm, center = TRUE,scale. = TRUE)
# nzac.norm.pc1<-data.frame(Date=nzac$Date,value=-nzac.norm.pca$x[,'PC1'])
# No!
#Z<-nzac.norm
#ZtZ<-t(Z)%*%Z
#PDP<-eigen(ZtZ)
## Rescale to GDP
# Table: Series, GDP(P), Chain volume, Seasonally adjusted, Total (Qrtly-Mar/Jun/Sep/Dec)
# SNEQ.SG01RSC00B01
# We take the APC then cut off as NZAC starts in 2003, but the serieses that feed into NZAC start in 2002
# Read in GDP
conn<-get_tsm_conn()
gdp<-tsmfetch(conn,"SNEQ.SG01RSC00B01") %>% rename(gdp=value) %>% mutate(Date=as.Date(as.yearqtr(paste(year,period), format = "%Y %q"))) %>%
mutate(gdp=round(gdp)) %>% mutate(l_gdp=100*(gdp/lag(gdp,4)-1)) %>% filter(Date>=min(nzac.pc1$Date))
# match time series length
nzac.mean<-mean(nzac.pc1[nzac.pc1$Date<=max(gdp$Date+months(2)),]$value)
nzac.sd<-sd(nzac.pc1[nzac.pc1$Date<=max(gdp$Date+months(2)),]$value)
gdp.mean<-mean(gdp$l_gdp,na.rm = TRUE)
gdp.sd<-sd(gdp$l_gdp,na.rm = TRUE)
nzac.pc1$scale<-(nzac.pc1$value-nzac.mean)*gdp.sd/nzac.sd+gdp.mean
#nzac.pc1$norm<-(nzac.pc1$value-nzac.mean)/nzac.sd
#nzac.pc1$try<-(nzac.pc1$value)*gdp.sd/nzac.sd+gdp.mean
write_xlsx(nzac.pc1, paste0(nzacdir,"NZAC portal data.xlsx"))
## READ IN SOURCE FILES
library(writexl)
library(readxl)
library(dplyr)
library(tidyr)
library(httr)
library(magrittr)
library(lubridate)
library(data.table)
source("tsmfetch.R")
options(digits=10)
dir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/"
nzacdir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/"
#meta<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/Raymund/META data/"
backcast<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/Backcast/"
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
###############################
### ANZ Activity Outlook
anz_outlook<-read_excel(paste0(nzacdir,"ANZ data/ANZ_Business_Outlook_Data.xlsx"), sheet = "ActOutlk",skip=4)
anz_activity_outlook<-anz_outlook%>%
select(Date=...1, activity_outlook=`TOTAL`)%>%
mutate(Date=ymd(Date)) %>% filter(activity_outlook!="...") %>% mutate(activity_outlook=as.numeric(activity_outlook))
# Filter non dates
# Impute Jan
minyear<-min(year(anz_activity_outlook$Date))
maxyear<-max(year(anz_activity_outlook$Date))
Dec<-seq.Date(from=as.Date(paste0(minyear,"/12/1")),to=as.Date(paste0(maxyear,"/12/1")),by="years")
DecVal<-anz_activity_outlook[anz_activity_outlook$Date %in% Dec,] %>%
mutate(Date=Date %m+% months(1)) %>% rename(decval=activity_outlook)
Feb<-seq.Date(from=as.Date(paste0(minyear,"/2/1")),to=as.Date(paste0(maxyear,"/2/1")),by="years")
FebVal<-anz_activity_outlook[anz_activity_outlook$Date %in% Feb,] %>%
mutate(Date=Date %m-% months(1)) %>% rename(febval=activity_outlook)
NewVal<-merge(DecVal,FebVal) %>% mutate(activity_outlook=(decval+febval)/2) %>% select(Date,activity_outlook)
# IMPUTE NEW JAN - Not an issue until next year!
anz_activity_outlook<-rbind(NewVal,anz_activity_outlook) %>% arrange(Date)
###############################
### BNZ PMI
#bnz_pmi_data<-read_excel(paste0(dir, "COVID 19 - BNZ insights.xlsx"),  sheet=1)
pmipage<-readLines("https://www.businessnz.org.nz/resources/surveys-and-statistics/pmi")
pmipage2<-pmipage[grep("PMI Time Series Data",pmipage)]
pmipage2<-pmipage2[grep("xls",pmipage2)]
pmi_url<-stringr::str_extract(pmipage2, url_pattern)
pmi_url<-pmi_url[1] # There are a few urls, we just want one.
#url <- "https://www.businessnz.org.nz/__data/assets/excel_doc/0003/73875/PMI-Time-Series-Data.xls"
GET(pmi_url, write_disk("pmi.xls", overwrite=TRUE))
bnz_pmi_data<-read_excel("pmi.xls",skip=1)
bnz_pmi<-bnz_pmi_data%>%select(Date="...1",pmi=PMI)%>%  mutate(Date=ymd(Date))
###############################
### ECT
ECT_data<-read_excel(paste0(dir,"COVID 19 - Electronic Card Transaction data.xlsx"), sheet = 1)
ECT<-ECT_data%>%
select(Date=...1,ECT_spend= `Total values - Electronic card transactions (Seasonally adjusted)`)%>%
mutate(Date=ymd(Date),year=year(Date),period=ceiling(month(Date)/3))
## DEFLATE SERIES
conn<-get_tsm_conn()
# RTD sales (nominal) ART9ZQ
rtd_nom<-tsmfetch(conn,"RTTQ.SF91CS") %>% rename(nom=value)
rtd_nom$nom<-round(rtd_nom$nom,1)
# RTD sales (real) ART9_ZQ
rtd_real<-tsmfetch(conn,"RTTQ.SF91KS") %>% rename(real=value)
rtd_real$real<-round(rtd_real$real,1)
# ART9ZQ/ART9_ZQ * ECT Nominal = ECT Real
# (if no deflator, use the monthly change from the previous year)
ect_deflator<-merge(rtd_nom,rtd_real) %>% mutate(deflate=nom/real)
ECT_real_in<-merge(ECT,ect_deflator,all.x=TRUE) %>% arrange(Date)
for(i in (nrow(ECT_real_in)-12):nrow(ECT_real_in)){
#Check for missing deflator. Should only be missing a few months until the next quarter is out, but check back over the past year
if(is.na(ECT_real_in[i,]$deflate)){
ECT_real_in[i,]$deflate<-ECT_real_in[i-12,]$deflate*(ECT_real_in[i-1,]$deflate/ECT_real_in[i-13,]$deflate)
}
}
ECT_real_in$ect_real<-ECT_real_in$ECT_spend/ECT_real_in$deflate
ECT_real<-ECT_real_in %>% select(Date,ect_real)
###############################
### Traffic
traffic<-read_excel(paste0(nzacdir,"ANZ data/ANZ Truckometer.xlsx"), sheet = "Data",skip=1)
traffic_portal<-traffic%>%select(Date=...1,heavy_traffic= `ANZ heavy traffic index...2`, light_traffic= `ANZ light traffic index...3`) %>%
mutate(Date=ymd(Date))
traffic_data<-traffic_portal
###############################
### Electricity Grid Demand
grid<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20040101&RegionType=NZ&TimeScale=MONTH&Source=RM",skip="Period start")
finaldemand<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20090101&RegionType=NZ&TimeScale=MONTH",skip="Period start")
electricity_back<-read.csv(paste0(backcast, "grid_demand.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
grid_electricity<-grid%>%
select(Date=`Period start`,grid_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date))
final_electricity<-finaldemand%>%
select(Date=`Period start`,final_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date)) %>% filter(Date>as.Date("2009-07-01"))
newdata<-data.frame()
if(max(final_electricity$Date)>max(grid_electricity$Date)){
# If there is a more recent month of Final Demand data available, we use linear regression to predict a grid demand figure
relate_data<-merge(grid_electricity,final_electricity)
relation<-lm(grid_demand~final_demand,data=relate_data)
newdata<-final_electricity[final_electricity$Date>max(grid_electricity$Date),]
newdata$grid_demand<-predict(relation,newdata)
newdata<-newdata %>% select(Date,grid_demand)
}
grid_electricity<-rbind(electricity_back,grid_electricity,newdata)
###############################
### Seek Job Ads
# Get latest file in directory
df <- file.info(list.files(paste0(nzacdir,"SEEK data"), full.names = T))
latest<-rownames(df)[which.max(df$mtime)]
jobs<-read_excel(latest, sheet = 1,skip=2)
jobs_back<-read.csv(paste0(backcast, "seek_job_ads.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
job_listing<-jobs%>%
select(Date, sk_job_ads=`NZ Job ad Index`)%>%
mutate(Date=ymd(Date))
# Need to rescale old job series based on (seasonally adjusted) first figure of more recent series
deflator<-jobs_back[jobs_back$Date==max(jobs_back$Date),]$sk_job_ads/
job_listing[job_listing$Date==min(job_listing$Date),]$sk_job_ads
jobs_back$sk_job_ads<-jobs_back$sk_job_ads/deflator
jobs_back<-jobs_back[!(jobs_back$Date==max(jobs_back$Date)),]
job_listing<-rbind(jobs_back,job_listing)
###############################
### Job Seeker
jobseek<-read_excel(paste0(dir,"COVID-19 MSD Jobseeker Support.xlsx"), sheet = 1)
jobseek_back<-read.csv(paste0(backcast, "jobseekers.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=ymd(Date),jobseekers=as.numeric(gsub(",", "", jobseekers)))
## Get Last Friday of Each Month Dates, 200 gives us until 2035
date.start.month=seq(as.Date("2020-01-01"),length=200,by="months")
date.end.month <- ceiling_date(date.start.month, "month")-1
Friday<-data.frame(
next_friday = ceiling_date(date.end.month, "week") + ifelse(weekdays(date.end.month) %in% c("Saturday", "Sunday"), 5, -2),
Date=rollbackward(date.end.month,roll_to_first = TRUE)
)
## Only get Last Friday numbers, and only for new rows
job_seekers_in<-jobseek%>%
select(next_friday='date', jobseekers=`Jobseeker Support - Work Ready`)%>%
mutate(next_friday=ymd(next_friday))
job_seekers<-merge(job_seekers_in,Friday) %>% select(Date,jobseekers) %>% filter(Date>max(jobseek_back$Date))
job_seekers<-rbind(jobseek_back,job_seekers)
## Update the backseries (as MSD only gives a Window of time)
write.csv(job_seekers,paste0(backcast, "jobseekers.csv"),row.names = FALSE)
#########################################
## FINAL DATA MERGE
all_data_merge<-Reduce(function(x,y) merge(x, y, all =T),list(bnz_pmi,anz_activity_outlook, job_seekers,
grid_electricity, traffic_data, job_listing, ECT_real))
#write_xlsx(all_data, paste0(nzacdir,"NZAC portal data ", Sys.Date(), ".xlsx"))
## READ IN SOURCE FILES
library(writexl)
library(readxl)
library(dplyr)
library(tidyr)
library(httr)
library(magrittr)
library(lubridate)
library(data.table)
source("tsmfetch.R")
options(digits=10)
dir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/"
nzacdir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/"
#meta<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/Raymund/META data/"
backcast<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/Backcast/"
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
###############################
### ANZ Activity Outlook
anz_outlook<-read_excel(paste0(nzacdir,"ANZ data/ANZ_Business_Outlook_Data.xlsx"), sheet = "ActOutlk",skip=4)
anz_activity_outlook<-anz_outlook%>%
select(Date=...1, activity_outlook=`TOTAL`)%>%
mutate(Date=ymd(Date)) %>% filter(activity_outlook!="...") %>% mutate(activity_outlook=as.numeric(activity_outlook))
# Filter non dates
# Impute Jan
minyear<-min(year(anz_activity_outlook$Date))
maxyear<-max(year(anz_activity_outlook$Date))
Dec<-seq.Date(from=as.Date(paste0(minyear,"/12/1")),to=as.Date(paste0(maxyear,"/12/1")),by="years")
DecVal<-anz_activity_outlook[anz_activity_outlook$Date %in% Dec,] %>%
mutate(Date=Date %m+% months(1)) %>% rename(decval=activity_outlook)
Feb<-seq.Date(from=as.Date(paste0(minyear,"/2/1")),to=as.Date(paste0(maxyear,"/2/1")),by="years")
FebVal<-anz_activity_outlook[anz_activity_outlook$Date %in% Feb,] %>%
mutate(Date=Date %m-% months(1)) %>% rename(febval=activity_outlook)
NewVal<-merge(DecVal,FebVal) %>% mutate(activity_outlook=(decval+febval)/2) %>% select(Date,activity_outlook)
# IMPUTE NEW JAN - Not an issue until next year!
anz_activity_outlook<-rbind(NewVal,anz_activity_outlook) %>% arrange(Date)
###############################
### BNZ PMI
#bnz_pmi_data<-read_excel(paste0(dir, "COVID 19 - BNZ insights.xlsx"),  sheet=1)
pmipage<-readLines("https://www.businessnz.org.nz/resources/surveys-and-statistics/pmi")
pmipage2<-pmipage[grep("PMI Time Series Data",pmipage)]
pmipage2<-pmipage2[grep("xls",pmipage2)]
pmi_url<-stringr::str_extract(pmipage2, url_pattern)
pmi_url<-pmi_url[1] # There are a few urls, we just want one.
#url <- "https://www.businessnz.org.nz/__data/assets/excel_doc/0003/73875/PMI-Time-Series-Data.xls"
GET(pmi_url, write_disk("pmi.xls", overwrite=TRUE))
bnz_pmi_data<-read_excel("pmi.xls",skip=1)
bnz_pmi<-bnz_pmi_data%>%select(Date="...1",pmi=PMI)%>%  mutate(Date=ymd(Date))
###############################
### ECT
ECT_data<-read_excel(paste0(dir,"COVID 19 - Electronic Card Transaction data.xlsx"), sheet = 1)
ECT<-ECT_data%>%
select(Date=...1,ECT_spend= `Total values - Electronic card transactions (Seasonally adjusted)`)%>%
mutate(Date=ymd(Date),year=year(Date),period=ceiling(month(Date)/3))
## DEFLATE SERIES
conn<-get_tsm_conn()
# RTD sales (nominal) ART9ZQ
rtd_nom<-tsmfetch(conn,"RTTQ.SF91CS") %>% rename(nom=value)
rtd_nom$nom<-round(rtd_nom$nom,1)
# RTD sales (real) ART9_ZQ
rtd_real<-tsmfetch(conn,"RTTQ.SF91KS") %>% rename(real=value)
rtd_real$real<-round(rtd_real$real,1)
# ART9ZQ/ART9_ZQ * ECT Nominal = ECT Real
# (if no deflator, use the monthly change from the previous year)
ect_deflator<-merge(rtd_nom,rtd_real) %>% mutate(deflate=nom/real)
ECT_real_in<-merge(ECT,ect_deflator,all.x=TRUE) %>% arrange(Date)
for(i in (nrow(ECT_real_in)-12):nrow(ECT_real_in)){
#Check for missing deflator. Should only be missing a few months until the next quarter is out, but check back over the past year
if(is.na(ECT_real_in[i,]$deflate)){
ECT_real_in[i,]$deflate<-ECT_real_in[i-12,]$deflate*(ECT_real_in[i-1,]$deflate/ECT_real_in[i-13,]$deflate)
}
}
ECT_real_in$ect_real<-ECT_real_in$ECT_spend/ECT_real_in$deflate
ECT_real<-ECT_real_in %>% select(Date,ect_real)
###############################
### Traffic
traffic<-read_excel(paste0(nzacdir,"ANZ data/ANZ Truckometer.xlsx"), sheet = "Data",skip=1)
traffic_portal<-traffic%>%select(Date=...1,heavy_traffic= `ANZ heavy traffic index...2`, light_traffic= `ANZ light traffic index...3`) %>%
mutate(Date=ymd(Date))
traffic_data<-traffic_portal
###############################
### Electricity Grid Demand
grid<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20040101&RegionType=NZ&TimeScale=MONTH&Source=RM",skip="Period start")
finaldemand<-fread("https://www.emi.ea.govt.nz/Wholesale/Download/DataReport/CSV/W_GD_C?DateFrom=20090101&RegionType=NZ&TimeScale=MONTH",skip="Period start")
electricity_back<-read.csv(paste0(backcast, "grid_demand.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
grid_electricity<-grid%>%
select(Date=`Period start`,grid_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date))
final_electricity<-finaldemand%>%
select(Date=`Period start`,final_demand= `Demand (GWh)`)%>%
mutate(Date=dmy(Date)) %>% filter(Date>as.Date("2009-07-01"))
newdata<-data.frame()
if(max(final_electricity$Date)>max(grid_electricity$Date)){
# If there is a more recent month of Final Demand data available, we use linear regression to predict a grid demand figure
relate_data<-merge(grid_electricity,final_electricity)
relation<-lm(grid_demand~final_demand,data=relate_data)
newdata<-final_electricity[final_electricity$Date>max(grid_electricity$Date),]
newdata$grid_demand<-predict(relation,newdata)
newdata<-newdata %>% select(Date,grid_demand)
}
grid_electricity<-rbind(electricity_back,grid_electricity,newdata)
###############################
### Seek Job Ads
# Get latest file in directory
df <- file.info(list.files(paste0(nzacdir,"SEEK data"), full.names = T))
latest<-rownames(df)[which.max(df$mtime)]
jobs<-read_excel(latest, sheet = 1,skip=2)
jobs_back<-read.csv(paste0(backcast, "seek_job_ads.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=dmy(Date))
job_listing<-jobs%>%
select(Date, sk_job_ads=`NZ Job ad Index`)%>%
mutate(Date=ymd(Date))
# Need to rescale old job series based on (seasonally adjusted) first figure of more recent series
deflator<-jobs_back[jobs_back$Date==max(jobs_back$Date),]$sk_job_ads/
job_listing[job_listing$Date==min(job_listing$Date),]$sk_job_ads
jobs_back$sk_job_ads<-jobs_back$sk_job_ads/deflator
jobs_back<-jobs_back[!(jobs_back$Date==max(jobs_back$Date)),]
job_listing<-rbind(jobs_back,job_listing)
###############################
### Job Seeker
jobseek<-read_excel(paste0(dir,"COVID-19 MSD Jobseeker Support.xlsx"), sheet = 1)
jobseek_back<-read.csv(paste0(backcast, "jobseekers.csv"), stringsAsFactors = FALSE) %>%
mutate(Date=ymd(Date),jobseekers=as.numeric(gsub(",", "", jobseekers)))
## Get Last Friday of Each Month Dates, 200 gives us until 2035
date.start.month=seq(as.Date("2020-01-01"),length=200,by="months")
date.end.month <- ceiling_date(date.start.month, "month")-1
Friday<-data.frame(
next_friday = ceiling_date(date.end.month, "week") + ifelse(weekdays(date.end.month) %in% c("Saturday", "Sunday"), 5, -2),
Date=rollback(date.end.month,roll_to_first = TRUE)
)
## Only get Last Friday numbers, and only for new rows
job_seekers_in<-jobseek%>%
select(next_friday='date', jobseekers=`Jobseeker Support - Work Ready`)%>%
mutate(next_friday=ymd(next_friday))
job_seekers<-merge(job_seekers_in,Friday) %>% select(Date,jobseekers) %>% filter(Date>max(jobseek_back$Date))
job_seekers<-rbind(jobseek_back,job_seekers)
## Update the backseries (as MSD only gives a Window of time)
write.csv(job_seekers,paste0(backcast, "jobseekers.csv"),row.names = FALSE)
#########################################
## FINAL DATA MERGE
all_data_merge<-Reduce(function(x,y) merge(x, y, all =T),list(bnz_pmi,anz_activity_outlook, job_seekers,
grid_electricity, traffic_data, job_listing, ECT_real))
#write_xlsx(all_data, paste0(nzacdir,"NZAC portal data ", Sys.Date(), ".xlsx"))
## SET UP LIBRARIES
library(writexl)
library(readxl)
library(dplyr)
library(tidyr)
library(magrittr)
library(lubridate)
library(data.table)
#library(nipals)
source("tsmfetch.R")
options(digits=10)
#If loaded libraries in read_in.R, need to add this.
library(zoo)
dir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/"
nzacdir<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/"
#meta<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/Raymund/META data/"
backcast<-"~/Network-Shares/U-Drive-SAS-03BAU/MEES/National Accounts/COVID-19 data_Secure/COVID-19_dashboard/NZAC/Backcast/"
## CREATE NZAC
all_data_merge<-Reduce(function(x,y) merge(x, y, all =T),list(bnz_pmi,anz_activity_outlook, job_seekers,
grid_electricity, traffic_data, job_listing, ECT_real))
all_data_merge$heavy_traffic<-round(all_data_merge$heavy_traffic,2)
all_data_merge$light_traffic<-round(all_data_merge$light_traffic,2)
# Create APC
all_data_merge$l_ect_real<-all_data_merge$ect_real/lag(all_data_merge$ect_real,12)-1
all_data_merge$l_heavy_traffic<-all_data_merge$heavy_traffic/lag(all_data_merge$heavy_traffic,12)-1
all_data_merge$l_light_traffic<-all_data_merge$light_traffic/lag(all_data_merge$light_traffic,12)-1
all_data_merge$l_grid_demand<-all_data_merge$grid_demand/lag(all_data_merge$grid_demand,12)-1
all_data_merge$l_jobseekers<-all_data_merge$jobseekers/lag(all_data_merge$jobseekers,12)-1
all_data_merge$l_sk_job_ads<-all_data_merge$sk_job_ads/lag(all_data_merge$sk_job_ads,12)-1
nzac<-all_data_merge %>% select(Date,l_ect_real,l_heavy_traffic,l_light_traffic,pmi,activity_outlook,l_grid_demand,l_jobseekers,l_sk_job_ads)
nzac<-nzac[nzac$Date>=as.Date("2003-10-01"),]
#nzac<-nzac[complete.cases(nzac),]
# Do PCA
#if(sum(is.na(nzac))>0){
#  nzac.pca <- empca(nzac[,c(2:9)], maxiter = 3000, tol=1e-5)
#  nzac.pc1<-data.frame(Date=nzac$Date,value=-nzac.pca$scores[,'PC1'])
#}else{
#}
nzac.pca <- prcomp(nzac[,c(2:9)], center = TRUE,scale. = TRUE)
nzac.pc1<-data.frame(Date=nzac$Date,value=-nzac.pca$x[,'PC1'])
# Does normalise do anything?
nzac.norm<-scale(nzac[,-c(1)])
# write.csv(nzac.norm,"normvalues.csv")
# nzac.norm.pca <- prcomp(nzac.norm, center = TRUE,scale. = TRUE)
# nzac.norm.pc1<-data.frame(Date=nzac$Date,value=-nzac.norm.pca$x[,'PC1'])
# No!
#Z<-nzac.norm
#ZtZ<-t(Z)%*%Z
#PDP<-eigen(ZtZ)
## Rescale to GDP
# Table: Series, GDP(P), Chain volume, Seasonally adjusted, Total (Qrtly-Mar/Jun/Sep/Dec)
# SNEQ.SG01RSC00B01
# We take the APC then cut off as NZAC starts in 2003, but the serieses that feed into NZAC start in 2002
# Read in GDP
conn<-get_tsm_conn()
gdp<-tsmfetch(conn,"SNEQ.SG01RSC00B01") %>% rename(gdp=value) %>% mutate(Date=as.Date(as.yearqtr(paste(year,period), format = "%Y %q"))) %>%
mutate(gdp=round(gdp)) %>% mutate(l_gdp=100*(gdp/lag(gdp,4)-1)) %>% filter(Date>=min(nzac.pc1$Date))
# match time series length
nzac.mean<-mean(nzac.pc1[nzac.pc1$Date<=max(gdp$Date+months(2)),]$value)
nzac.sd<-sd(nzac.pc1[nzac.pc1$Date<=max(gdp$Date+months(2)),]$value)
gdp.mean<-mean(gdp$l_gdp,na.rm = TRUE)
gdp.sd<-sd(gdp$l_gdp,na.rm = TRUE)
nzac.pc1$scale<-(nzac.pc1$value-nzac.mean)*gdp.sd/nzac.sd+gdp.mean
#nzac.pc1$norm<-(nzac.pc1$value-nzac.mean)/nzac.sd
#nzac.pc1$try<-(nzac.pc1$value)*gdp.sd/nzac.sd+gdp.mean
write_xlsx(nzac.pc1, paste0(nzacdir,"NZAC portal data.xlsx"))
source("scripts/load_data.R")
CONFIG <- read_config_file()
load_data(CONFIG, odata_load_flag=TRUE)
source("scripts/load_data.R")
CONFIG <- read_config_file()
load_data(CONFIG, odata_load_flag=TRUE)
